---
title: "Weekly Summary Template"
author: "Brady Miller"
title-block-banner: true
title-block-style: default
toc: true
#format: html
format: pdf
---

---

## Tuesday, March 14

::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. More with k-fold cross validation
1. Using caret package with cross validation
1. Using caret package with LASSO regression
:::

```{r}
# opening some necessary libraries
library(ISLR2)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(glmnet)
library(caret)
library(car)
library(torch)
```

```{r}
# opening Boston data set which will be used in examples
df <- Boston
attach(Boston)
```

#### k-fold cross validation

```{r}
k <- 5
folds <- sample(1:k, nrow(df), replace = T)

df_folds <- list()

# define list of data frame where every list has train and test
for (i in 1:k){
  df_folds[[i]] <- list()
  df_folds[[i]]$train = df[which(folds == i), ]
}
```


```{r}
# finding the mean squared error for each fold variation
kfold_mspe <- c()
for (i in 1:k) {
  model <- lm(medv ~ ., df_folds[[i]]$train)
  y_hat <- predict(model, df_folds[[i]]$test)
  kfold_mspe[i] <- mean((y_hat - df_folds[[i]]$test$medv)^2)
}
```

```{r}
make_folds <- function(df, k){
  folds <- sample(1:k, nrow(df), replace = T)
  df_folds <- list()
  for (i in 1:k){
    df_folds[[i]] <- list()
    df_folds[[i]]$train = df[which(folds != i), ]
    df_folds[[i]]$test = df[which(folds == i), ]
  }
  return(df_folds)
}
```


```{r}
# cross validation mean squared prediction error function
cv_mspe <- function(formula, df_folds){
  kfold_mspe <- c()
  # going through each fold to generate predictions and find the error for each
  for (i in 1:length(df_folds)){
    model <- lm(formula, df_folds[[i]]$train)
    y_hat <- predict(model, df_folds[[i]]$test)
    kfold_mspe <- mean((y_hat - df_folds[[i]]$test$medv)^2)
  }
  return(mean(kfold_mspe))
}
```
* This function can then be used to find the prediction error generated for a 
given set of variables over a certain amount of folds. This can help you 
determine which set of variables would be best to create the least error



#### cross validation with caret package

By using the caret package, we can write code to cross validate a dataset in
much fewer lines as shown below

```{r}
# specifying you want to cross validate using 5 folds (the number)
ctrl <- trainControl(method = 'cv', number = 5)
ctrl
```

```{r}
# creating model that uses linear regression model to predict med. house price
# trControl attribute uses the ctrl object to specify 5-fold cross validation
model <- train(medv ~ ., data = df, method = 'lm', trControl = ctrl)
summary(model)
```
This model created by cross validation using the caret package indicates that 
the age and indus variables are not significant in predicting the median house
price of a given home. The $R^2$ value indicates that there is a somewhat strong
positive correlation between the covariates and the outcome variable.

```{r}
# creates predictions for each piece of data in the Boston data set
predictions <- predict(model,df)
sample(predictions,10)
```
Above we show 5 randomly selected predictions made on the data set. Each row 
number has the associated predicted median house value with it. 



#### LASSO regression with caret package
 
This deals with bias-variance trade off. As it states there is a trade off 
between having more bias or more variance. By having a model that has more 
variables, bias decreases but variance increases. Having too much variance can
lead to overfitting data and create good performance on train data but bad 
performance on test data. On the other hand, a model with only a few variables
increases bias and reduces variance. Increasing bias can lead to creating a 
model that underfits data, resulting in poor performance on both the train and 
test data. This relates to LASSO as it is a form of variable selection, so you 
want to make sure you are selecting the right amount of variables so you have a 
balanced trade off between bias and variance

```{r}
ctrl <- trainControl(method = 'cv', number = 5)

# Defining the tuning grid
grid <- expand.grid(alpha = 1, lambda = seq(0, 0.1, by = 0.005))

# Train the model using LASSO regression with cross validation
lasso_fit <- train(medv ~ ., data = df, method = 'glmnet', trControl = ctrl,
                   tuneGrid = grid, standardize = TRUE, family = 'gaussian')

plot(lasso_fit)
```
This plot shows the mean squared error value for each regularization parameter 
tried in the LASSO regression with the cross validation. From this, we can see 
that the regularization parameter that minimizes the mean squared error value 
is at about 0.03, so $\lambda$ should be 0.03 when doing LASSO regression.



## Thursday, March 16



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Item 1
1. Item 2
1. Item 3
:::



#### Classification

$$
\boxed {y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p}
$$

Looking at different loss functions
1. Least sqaures
  $L(\beta) = \sum_{i=1}^n \|y_i - \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p|$

1. Penalized least sqaures/LASSO 
  $L(\beta) = \sum_{i=1}^n \|y_i - \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p||^2 + \lambda\|\beta||$
  
*Will move to categorical response variables and generalization of the right side using neural networks
*Combine left and right side to make neural networks that perform classification

```{r}
url <- 'https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)'
col_names <- c('id', 'diagnosis', paste0('feat', 1:30))
df <- read_csv(
  url, col_names, col_types = cols()
) %>%
  select(-id) %>%
  mutate(outcome = ifelse('diagnosis' == 'M', 1, 0)) %>%
  select(-diagnosis)
```


```r
reg_model <- lm(diagnosis ~ ., data = df)
```
Most features are not significant


```r
n <- 100
new_patients <- data.frame(matrix(rnomr(30 * n), nrow = n))
colnames(new_patients) <- paste0('feat', 1:30)
new_predictions <- predict(reg_model, newdata = new_patients, type = 'response'0)
```

```r
print(new_predictions %>% head())
```
* responses are 0 or 1 (tumor is malignant or benign), but what we get from this is values that don't make sense --> can't represent probability that someones tumor is malignant
 
* logistic regression would be better as logistic regression in this model can end up giving negative numbers which doesn't make sense (meaningless predictions)

* linear regression for binary models will give you bad values for extrapolation


```r
boxplot(new_predictions)
```
shows that there are bad/inaccurate responses


#### Logistic regression

*odds = p/(1-p)

*useful to interpret for binary responses

*if p is between (0,1) then odds is between (0, infinity)

*log-odds = log(odds)
  1. log(p/(1-p))
  1. log-odds takes values between (-infinity, infinity)
  1. creates continuous scale which you can do linear regression on            (transforming scale so you can do linear reg, and then transforming it back to get value)

* will specify a regression model and then use log-odds to get back what the probabilities are

$$
log-odds(\beta(x)) = \beta_0 + \beta_1 x_1 ->

p(x) = \frac{1}{1+exp(\beta_0 + \beta_1 x_1)}
$$

*p/(1-p) = e^($\beta_0$ + $\beta_1$x$x_1$)
* p(x) = e^($\beta_0$ + $\beta_1$x$x_1$) / (1 + e^($\beta_0$ + $\beta_1$x$x_1$))
* p(x) = 1/( (1/e^-($\beta_0$ + $\beta_1$x$x_1$)) + 1)

* want to take 


SIGMOID FUNCTION - logistic regression
```{r}
sigmoid <- \(x) 1/(1+exp(-x))
curve(sigmoid, -100, 100, ylab = 'sigmoid(x)')
```


p(x) = sigmoid(\beta_0 + \beta_1 x_1) = \frac{1}{1+exp(-\beta_0 - \beta_1 x_1)}


###### Example

```{r}
set.seed(123)
x <- rnorm(100)
y <- rbinom(100, size = 1, prob=exp(0.5 + 0.8*x)/(1+exp(0.5 +0.8*x)))
```

```{r}
y
```

```{r}
model <- glm(y ~ x, family = binomial())
summary(model)
```
the ground truth (model actually generating from) has intercept 0.5 and slope 0.8

```{r}
x_test <- -5.5
sigmoid(coef(model)[1] + coef(model)[2] + x_test)

predict(model, newdata = data.frame(x=x_test), type = 'response')
```

```{r}
new_x <- seq(-2,2, by=0.1)
p1 <- predict(model, data.frame(x=new_x))
p2 <- predict(model, data.frame(x=new_x), type = 'response')

boxplot(p1,p2)
```

Logistic regression for breast cancer data set
```r
df <- df %>% mutate_at('outcome', factor)

model <- glm(outcome ~., data =df, family = binomial())
summary(model)
```
*stuff we initially thought were insignificant are now shown to be important
*feature 7 is not significant at all --> is actually significant, just very collinear with another variable
* logistic regression suffers from multicollinearity 


* There is no $R^2$ value for logistic regression as there is for linear regression --> no line fitting x to y
* computes deviance instead (similar but different)
* Also shows AIC



###### Logistic regression using torch library
```{r}
X <- cbind(x)
x_tensor <- 
y_tensor <- 
```

* neural network module --> in module you want to construct, there is some internal building step where you specify the structure
* first layer in linear regression layer (input and output layer) (x and y are 1-dimensional)
*can use module whenever you want to
*an appropriate loss function would be
