---
title: "Weekly Summary Template"
author: "Brady Miller"
title-block-banner: true
title-block-style: default
toc: true
#format: html
format: pdf
---

---

## Tuesday, March 14

::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. More with k-fold cross validation
1. Using caret package with cross validation
1. Using caret package with LASSO regression
:::

```{r}
# opening some necessary libraries
library(ISLR2)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(glmnet)
library(caret)
library(car)
library(torch)
```

```{r}
# opening Boston data set which will be used in examples
df <- Boston
attach(Boston)
```

#### k-fold cross validation

```{r}
k <- 5
folds <- sample(1:k, nrow(df), replace = T)

df_folds <- list()

# define list of data frame where every list has train and test
for (i in 1:k){
  df_folds[[i]] <- list()
  df_folds[[i]]$train = df[which(folds == i), ]
}
```


```{r}
# finding the mean squared error for each fold variation
kfold_mspe <- c()
for (i in 1:k) {
  model <- lm(medv ~ ., df_folds[[i]]$train)
  y_hat <- predict(model, df_folds[[i]]$test)
  kfold_mspe[i] <- mean((y_hat - df_folds[[i]]$test$medv)^2)
}
```

```{r}
make_folds <- function(df, k){
  folds <- sample(1:k, nrow(df), replace = T)
  df_folds <- list()
  for (i in 1:k){
    df_folds[[i]] <- list()
    df_folds[[i]]$train = df[which(folds != i), ]
    df_folds[[i]]$test = df[which(folds == i), ]
  }
  return(df_folds)
}
```


```{r}
# cross validation mean squared prediction error function
cv_mspe <- function(formula, df_folds){
  kfold_mspe <- c()
  # going through each fold to generate predictions and find the error for each
  for (i in 1:length(df_folds)){
    model <- lm(formula, df_folds[[i]]$train)
    y_hat <- predict(model, df_folds[[i]]$test)
    kfold_mspe <- mean((y_hat - df_folds[[i]]$test$medv)^2)
  }
  return(mean(kfold_mspe))
}
```
* This function can then be used to find the prediction error generated for a 
given set of variables over a certain amount of folds. This can help you 
determine which set of variables would be best to create the least error



#### cross validation with caret package

By using the caret package, we can write code to cross validate a dataset in
much fewer lines as shown below

```{r}
# specifying you want to cross validate using 5 folds (the number)
ctrl <- trainControl(method = 'cv', number = 5)
ctrl
```

```{r}
# creating model that uses linear regression model to predict med. house price
# trControl attribute uses the ctrl object to specify 5-fold cross validation
model <- train(medv ~ ., data = df, method = 'lm', trControl = ctrl)
summary(model)
```
This model created by cross validation using the caret package indicates that 
the age and indus variables are not significant in predicting the median house
price of a given home. The $R^2$ value indicates that there is a somewhat strong
positive correlation between the covariates and the outcome variable.

```{r}
# creates predictions for each piece of data in the Boston data set
predictions <- predict(model,df)
sample(predictions,10)
```
Above we show 5 randomly selected predictions made on the data set. Each row 
number has the associated predicted median house value with it. 



#### LASSO regression with caret package
 
This deals with bias-variance trade off. As it states there is a trade off 
between having more bias or more variance. By having a model that has more 
variables, bias decreases but variance increases. Having too much variance can
lead to overfitting data and create good performance on train data but bad 
performance on test data. On the other hand, a model with only a few variables
increases bias and reduces variance. Increasing bias can lead to creating a 
model that underfits data, resulting in poor performance on both the train and 
test data. This relates to LASSO as it is a form of variable selection, so you 
want to make sure you are selecting the right amount of variables so you have a 
balanced trade off between bias and variance

```{r}
ctrl <- trainControl(method = 'cv', number = 5)

# Defining the tuning grid
grid <- expand.grid(alpha = 1, lambda = seq(0, 0.1, by = 0.005))

# Train the model using LASSO regression with cross validation
lasso_fit <- train(medv ~ ., data = df, method = 'glmnet', trControl = ctrl,
                   tuneGrid = grid, standardize = TRUE, family = 'gaussian')

plot(lasso_fit)
```
This plot shows the mean squared error value for each regularization parameter 
tried in the LASSO regression with the cross validation. From this, we can see 
that the regularization parameter that minimizes the mean squared error value 
is at about 0.03, so $\lambda$ should be 0.03 when doing LASSO regression.



## Thursday, March 16



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Why linear regression won't work with binary/categorical response variables 
   for classification problems
1. Logistic regression using sigmoid function
1. Interpreting logistic regression/model summary with a real data set
:::


#### Why linear regression won't work for binary/categorical classification

```{r}
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'
col_names <- c('id', 'diagnosis', paste0('feat', 1:30))
df <- read_csv(
  url, col_names, col_types = cols()
) %>%
  select(-id) %>%
  mutate(outcome = ifelse(diagnosis == 'M', 1, 0)) %>%
  select(-diagnosis)
```


```{r}
reg_model <- lm(outcome ~ ., data = df)
summary
```
Model summary indicates that most features are not significant


```{r}
n <- 100
new_patients <- data.frame(matrix(rnorm(30 * n), nrow = n))
colnames(new_patients) <- paste0('feat', 1:30)
new_predictions <- predict(reg_model, newdata = new_patients, type = 'response')
```

```{r}
print(new_predictions %>% head())
```
* The responses should be 0 or 1 (tumor is malignant or benign), but what we get
  from this is values that don't make sense --> can't represent probability that
  someones tumor is malignant
 
* logistic regression would be better as this model can end up giving negative 
  numbers which doesn't make sense (meaningless predictions)

* linear regression for binary models will give you bad values for extrapolation


```{r}
boxplot(new_predictions)
```
The range of values going negative shows that there are bad/inaccurate responses





#### Logistic regression using sigmoid function

*odds = p/(1-p)

*useful to interpret for binary responses

*if p is between (0,1) then odds is between (0, infinity)

*log-odds = log(odds)
  1. log(p/(1-p))
  1. log-odds takes values between (-infinity, infinity)
  1. creates continuous scale which you can do linear regression on 
     (transforming scale so you can do linear reg, and then transforming it back
     to get value)

* will specify a regression model and then use log-odds to get back what the 
  probabilities are

$$
\frac{p}{1-p} = exp(\beta_0 + \beta_1 x_1)
$$
$$
LogOdds(\beta(x)) = \beta_0 + \beta_1 x_1
$$

$$
p(x) =  \frac{exp(\beta_0 + \beta_1 x_1)}{1+exp(\beta_0 + \beta_1 x_1)} = \frac{1}{1+exp(\beta_0 + \beta_1 x_1)}
$$

sigmoid function - logistic regression
```{r}
sigmoid <- \(x) 1/(1+exp(-x))
curve(sigmoid, -100, 100, ylab = 'sigmoid(x)')
```

$$
p(x) = sigmoid(\beta_0 + \beta_1 x_1) = \frac{1}{1+exp(-\beta_0 - \beta_1 x_1)}
$$

#### Interpreting Logistic Regression for Breast Cancer Data Set
```{r}
df <- df %>% mutate_at('outcome', factor)

model <- glm(outcome ~., data =df, family = binomial())
summary(model)
```
* Variables we initially thought were insignificant are now shown to be important
* Feature 7 is not significant at all --> is actually significant, just very
  collinear with another variable
* Logistic regression suffers from multicollinearity 
* There is no $R^2$ value for logistic regression as there is for linear
  regression --> no line fitting x to y
* Computes deviance instead (similar but different)
* Also shows AIC